{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone git@github.com:hnoya/prj-kaggle-retrospection.git\n",
    "\n",
    "%cd prj-kaggle-retrospection/2022-NBME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 次のセルの`$YOUR_JSON_PATH`は自身のkaggle.jsonのパスに置き換える\n",
    "    - e.g. `\"/content/drive/MyDrive/kaggle.json\"` など"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q kaggle\n",
    "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
    "\n",
    "!mkdir ~/.kaggle\n",
    "!cp $YOUR_JSON_PATH ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "%cd data\n",
    "\n",
    "!kaggle competitions download -c nbme-score-clinical-patient-notes\n",
    "!unzip -q nbme-score-clinical-patient-notes.zip\n",
    "\n",
    "%cd ..\n",
    "\n",
    "!mkdir weights\n",
    "\n",
    "!mkdir tokenizer_codes\n",
    "%cd tokenizer_codes\n",
    "!kaggle datasets download -d thanhns/deberta-v2-3-fast-tokenizer\n",
    "!unzip deberta-v2-3-fast-tokenizer.zip\n",
    "%cd ..\n",
    "\n",
    "!mkdir deberta-tokenizer\n",
    "%cd deberta-tokenizer\n",
    "!kaggle datasets download -d thanhns/deberta-tokenizer\n",
    "!unzip deberta-tokenizer.zip\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "transformers_path = Path(\"/usr/local/lib/python3.7/dist-packages/transformers\")\n",
    "\n",
    "input_dir = Path(\"./tokenizer_codes\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    \n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 次のセルの`$YOUR_WEIGHT_PATH`は自分で作成した学習済みモデルを置くパス\n",
    "    - e.g. `\"/content/drive/MyDrive/kaggle/2022-NBME/weights/\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r model.zip ./weights/*\n",
    "!cp model.zip $YOUR_WEIGHT_PATH"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
